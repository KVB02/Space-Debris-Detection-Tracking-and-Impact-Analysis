{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed3717b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the base file path and template\n",
    "base_path = \"tle_data_first\" \n",
    "file_template = \"_{}.csv\"\n",
    "\n",
    "# Generate a list of file paths\n",
    "file_paths = [os.path.join(base_path, file_template.format(i)) for i in range(1000, 59000, 1000)]\n",
    "\n",
    "# Initialize an empty list to store DataFrames\n",
    "dfs = []\n",
    "\n",
    "\n",
    "for file_path in file_paths:\n",
    "    file_path = file_path.replace('/', '')\n",
    "    # Since we're joining paths, there's no need to remove '/' from file_path\n",
    "    if os.path.exists(file_path) and os.path.getsize(file_path) > 0:  \n",
    "        print(f\"Reading file: {file_path}\")  # Print the file path for debugging\n",
    "        # Read the file with the correct delimiter and no header assumption\n",
    "        df = pd.read_csv(file_path, delimiter=',', skiprows = 1, header=None, encoding='utf-8')\n",
    "        # Ensure the DataFrame has the correct column names\n",
    "        df.columns = ['NORAD_CAT_ID', 'TLE_LINE1', 'TLE_LINE2']\n",
    "        dfs.append(df)\n",
    "\n",
    "# Merge all non-empty DataFrames\n",
    "if dfs:\n",
    "    merged_df = pd.concat(dfs, ignore_index=True)\n",
    "else:\n",
    "    print(\"No valid data files found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89a21a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Timestamp from the TLE\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "#lists to store the extracted data\n",
    "exact_timestamps = []\n",
    "\n",
    "# Select the number of batches based on the batch size\n",
    "batch_size = 10000\n",
    "total_rows = merged_df.shape[0]\n",
    "num_batches = (total_rows // batch_size) + (1 if total_rows % batch_size != 0 else 0)\n",
    "\n",
    "for batch_num in range(num_batches):\n",
    "    start_index = batch_num * batch_size\n",
    "    end_index = min((batch_num + 1) * batch_size, total_rows)\n",
    "    print(f\"Processing batch {batch_num + 1}/{num_batches}, Rows: {start_index} to {end_index}\")\n",
    "\n",
    "    \n",
    "    for index in range(start_index, end_index):\n",
    "        row = merged_df.iloc[index]\n",
    "\n",
    "        # Extract year and full epoch day (including fractional day) from TLE line\n",
    "        year_str = row['TLE_LINE1'][18:20].strip()\n",
    "        epoch_day_str = row['TLE_LINE1'][20:32].strip()  \n",
    "        \n",
    "        # Convert to integers, adjusting for century\n",
    "        year = int(year_str) + (1900 if int(year_str) >= 57 else 2000)\n",
    "\n",
    "        # Split epoch day into day of year and fractional day\n",
    "        try:\n",
    "            day_of_year, frac_day_str = epoch_day_str.split('.')\n",
    "            day_of_year = int(day_of_year)\n",
    "            frac_day = float('0.' + frac_day_str)\n",
    "\n",
    "            # Calculate the date from year and day of year\n",
    "            date = datetime.datetime(year, 1, 1) + datetime.timedelta(days=day_of_year - 1)\n",
    "\n",
    "            # Convert fractional day to hours, minutes, and seconds\n",
    "            total_seconds = int(frac_day * 86400)\n",
    "            hours = total_seconds // 3600\n",
    "            minutes = (total_seconds % 3600) // 60\n",
    "            seconds = total_seconds % 60\n",
    "\n",
    "            # Combine date and time into a single datetime object\n",
    "            exact_timestamp = datetime.datetime(year, date.month, date.day, hours, minutes, seconds)\n",
    "            exact_timestamps.append(exact_timestamp)\n",
    "        except ValueError as e:\n",
    "            print(f\"Error at index {index}: {e}, Line1: {row['TLE_LINE1']}\")\n",
    "            exact_timestamps.append(None)\n",
    "\n",
    "# Add the extracted data to the DataFrame\n",
    "merged_df['Exact_Timestamp'] = exact_timestamps\n",
    "merged_df['Exact_Date'] = merged_df['Exact_Timestamp'].dt.date\n",
    "merged_df['Exact_Time'] = merged_df['Exact_Timestamp'].dt.time\n",
    "\n",
    "\n",
    "sorted_df = merged_df.sort_values(by=['NORAD_CAT_ID', 'Exact_Timestamp'])\n",
    "\n",
    "\n",
    "print(sorted_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5c36d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "missing_data = sorted_df.isnull().sum()\n",
    "print(missing_data[missing_data > 0])  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32465adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Orbital Parameters\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Add a new column for flagging corrupted data\n",
    "sorted_df['Corrupted'] = False\n",
    "\n",
    "def extract_and_validate_tle_elements(row):\n",
    "    # Extracting elements from TLE_LINE2 and initializing flag as False\n",
    "    elements = {'inclination': float(row['TLE_LINE2'][8:16]),\n",
    "                'raan': float(row['TLE_LINE2'][17:25]),\n",
    "                'eccentricity': float('0.' + row['TLE_LINE2'][26:33]),\n",
    "                'argument_of_perigee': float(row['TLE_LINE2'][34:42]),\n",
    "                'mean_anomaly': float(row['TLE_LINE2'][43:51])}\n",
    "    corrupted = False\n",
    "    \n",
    "    # Range checks with flagging\n",
    "    if not 0 <= elements['inclination'] <= 180:\n",
    "        corrupted = True\n",
    "    if not 0 <= elements['raan'] <= 360:\n",
    "        corrupted = True\n",
    "    if not 0 <= elements['eccentricity'] < 1:\n",
    "        corrupted = True\n",
    "    if not 0 <= elements['argument_of_perigee'] <= 360:\n",
    "        corrupted = True\n",
    "    if not 0 <= elements['mean_anomaly'] <= 360:\n",
    "        corrupted = True\n",
    "    \n",
    "    return corrupted\n",
    "\n",
    "# flag corrupted entries\n",
    "for index, row in sorted_df.iterrows():\n",
    "    if extract_and_validate_tle_elements(row):\n",
    "        sorted_df.at[index, 'Corrupted'] = True\n",
    "\n",
    "\n",
    "corrupted_data = sorted_df[sorted_df['Corrupted'] == True]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3098ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mean_motion(tle_line2):\n",
    "    # Extracting mean motion from TLE_LINE2\n",
    "    return float(tle_line2[52:63].strip())\n",
    "\n",
    "# extract mean motion for each row\n",
    "sorted_df['mean_motion'] = sorted_df['TLE_LINE2'].apply(extract_mean_motion)\n",
    "\n",
    "\n",
    "def extract_eccentricity(tle_line2):\n",
    "    # Eccentricity is given starting from the 27th character in TLE_LINE2 for 7 characters\n",
    "    # A leading decimal point is assumed, so it's added manually\n",
    "    eccentricity_str = tle_line2[26:33].strip()\n",
    "    eccentricity = float('0.' + eccentricity_str)\n",
    "    return eccentricity\n",
    "\n",
    "# extract eccentricity for each row and create the 'eccentricity' column\n",
    "sorted_df['eccentricity'] = sorted_df['TLE_LINE2'].apply(extract_eccentricity)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Gravitational constant for Earth (mu = GM, in km^3/s^2)\n",
    "mu_earth = 398600.4418  \n",
    "\n",
    "def calculate_semi_major_axis(mean_motion):\n",
    "    # Convert mean motion from revs per day to radians per second\n",
    "    mean_motion_rad_s = mean_motion * 2 * np.pi / 86400\n",
    "    # Calculate semi-major axis from mean motion\n",
    "    a = (mu_earth / (mean_motion_rad_s ** 2)) ** (1/3)\n",
    "    return a  # in kilometers\n",
    "\n",
    "def calculate_orbital_period(semi_major_axis):\n",
    "    # Calculate orbital period from semi-major axis\n",
    "    T = 2 * np.pi * np.sqrt(semi_major_axis**3 / mu_earth)\n",
    "    return T  # in seconds\n",
    "\n",
    "\n",
    "sorted_df['Semi_Major_Axis'] = sorted_df['mean_motion'].apply(calculate_semi_major_axis)\n",
    "sorted_df['Orbital_Period'] = sorted_df['Semi_Major_Axis'].apply(calculate_orbital_period)\n",
    "\n",
    "def calculate_apogee_perigee(semi_major_axis, eccentricity):\n",
    "    apogee = semi_major_axis * (1 + eccentricity) - 6371  # Earth's radius in km subtracted to get altitude\n",
    "    perigee = semi_major_axis * (1 - eccentricity) - 6371\n",
    "    return apogee, perigee\n",
    "\n",
    "\n",
    "sorted_df[['Apogee', 'Perigee']] = sorted_df.apply(lambda row: calculate_apogee_perigee(row['Semi_Major_Axis'], row['eccentricity']), axis=1, result_type='expand')\n",
    "\n",
    "\n",
    "from skyfield.api import load, EarthSatellite\n",
    "import numpy as np\n",
    "\n",
    "ts = load.timescale()\n",
    "\n",
    "def extract_orbital_elements(tle_line1, tle_line2):\n",
    "    satellite = EarthSatellite(tle_line1, tle_line2)\n",
    "    \n",
    "    inclination = satellite.model.inclo  # Inclination in radians\n",
    "    raan = satellite.model.nodeo  # Right Ascension of Ascending Node in radians\n",
    "    \n",
    "    inclination_deg = np.degrees(inclination)\n",
    "    raan_deg = np.degrees(raan)\n",
    "    \n",
    "    return inclination_deg, raan_deg\n",
    "\n",
    "def add_additional_params(df):\n",
    "    inclinations, raans = [], []\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        inclination, raan = extract_orbital_elements(row['TLE_LINE1'], row['TLE_LINE2'])\n",
    "        inclinations.append(inclination)\n",
    "        raans.append(raan)\n",
    "    \n",
    "    df['Inclination'] = inclinations\n",
    "    df['RAAN'] = raans\n",
    "    return df\n",
    "\n",
    "\n",
    "sorted_df = add_additional_params(sorted_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28868589",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_df.rename(columns={'Exact_Timestamp':'Timestamp','Exact_Date':'Epoch_Date','Exact_Time':'Epoch_Time','mean_motion':'Mean_Motion','eccentricity':'Eccentricity'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c7ff14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Latitude and Lngitude using SKyfield library\n",
    "\n",
    "\n",
    "from skyfield.api import load, EarthSatellite\n",
    "from datetime import datetime\n",
    "\n",
    "ts = load.timescale()\n",
    "\n",
    "def extract_lat_lon_alt(tle_line1, tle_line2, epoch_date, epoch_time):\n",
    "    \n",
    "    # Combine date and time into a single datetime object\n",
    "    epoch_datetime = datetime.strptime(f'{epoch_date} {epoch_time}', '%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    satellite = EarthSatellite(tle_line1, tle_line2)\n",
    "    time = ts.utc(epoch_datetime.year, epoch_datetime.month, epoch_datetime.day, \n",
    "                  epoch_datetime.hour, epoch_datetime.minute, epoch_datetime.second)\n",
    "    geocentric = satellite.at(time)\n",
    "    subpoint = geocentric.subpoint()\n",
    "    distance_from_center = geocentric.distance().km  # Distance from the Earth's center in kilometers\n",
    "    altitude = distance_from_center - 6371  # Subtracting average Earth radius to get altitude above surface\n",
    "    \n",
    "    return subpoint.latitude.degrees, subpoint.longitude.degrees, altitude\n",
    "\n",
    "def add_orbital_params(df, batch_size=10000):\n",
    "    latitudes = []\n",
    "    longitudes = []\n",
    "    altitudes = []\n",
    "    \n",
    "    # Process data in batches\n",
    "    for index in range(0, len(df), batch_size):\n",
    "        batch_df = df.iloc[index:index+batch_size]\n",
    "        for _, row in batch_df.iterrows():\n",
    "            lat, lon, alt = extract_lat_lon_alt(row['TLE_LINE1'], row['TLE_LINE2'], \n",
    "                                                 row['Epoch_Date'], row['Epoch_Time'])\n",
    "            latitudes.append(lat)\n",
    "            longitudes.append(lon)\n",
    "            altitudes.append(alt)\n",
    "        print(f'Processed {index+batch_size} records')\n",
    "\n",
    "    df['Latitude'] = latitudes\n",
    "    df['Longitude'] = longitudes\n",
    "    df['Altitude'] = altitudes\n",
    "    return df\n",
    "\n",
    "\n",
    "sorted_df = add_orbital_params(sorted_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beda68c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering the data with each Norad ID having 1000 records\n",
    "\n",
    "\n",
    "#Selecting only the 1000 records\n",
    "\n",
    "\n",
    "filtered_df = sorted_df.groupby('NORAD_CAT_ID').filter(lambda x: len(x) >= 1000)\n",
    "\n",
    "print(filtered_df.shape,filtered_df['NORAD_CAT_ID'].nunique())\n",
    "\n",
    "filtered_df = filtered_df.groupby('NORAD_CAT_ID').head(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08a5d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding additional Features\n",
    "\n",
    "\n",
    "# Convert 'Timestamp' column to datetime \n",
    "filtered_df['Timestamp'] = pd.to_datetime(filtered_df['Timestamp'])\n",
    "\n",
    "# Ensure the DataFrame is sorted by 'Timestamp' to correctly compute differences\n",
    "filtered_df.sort_values(by=['NORAD_CAT_ID','Timestamp'], inplace=True)\n",
    "\n",
    "# Calculate time differences between consecutive records in seconds\n",
    "filtered_df['Time_Diffs'] = filtered_df['Timestamp'].diff().dt.total_seconds().fillna(0)\n",
    "\n",
    "# Calculate changes in position (differences) for Latitude, Longitude, and Altitude\n",
    "filtered_df['Lat_Diffs'] = filtered_df['Latitude'].diff().fillna(0)\n",
    "filtered_df['Lon_Diffs'] = filtered_df['Longitude'].diff().fillna(0)\n",
    "filtered_df['Alt_Diffs'] = filtered_df['Altitude'].diff().fillna(0)\n",
    "\n",
    "# Compute velocity components by dividing position changes by time differences\n",
    "filtered_df['Vel_Latitude'] = filtered_df['Lat_Diffs'] / filtered_df['Time_Diffs']\n",
    "filtered_df['Vel_Longitude'] = filtered_df['Lon_Diffs'] / filtered_df['Time_Diffs']\n",
    "filtered_df['Vel_Altitude'] = filtered_df['Alt_Diffs'] / filtered_df['Time_Diffs']\n",
    "\n",
    "# Handle potential divide by zero or NaN issues from fillna(0)\n",
    "filtered_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "filtered_df.fillna(0, inplace=True)  # Setting NaNs to 0; adjust based on your needs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434cabfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_bstar(tle_line):\n",
    "    # Extract the BSTAR string from the TLE line\n",
    "    bstar_str = tle_line[53:61].strip()\n",
    "\n",
    "    # Check for the presence of an exponent sign (+ or -)\n",
    "    if ' ' in bstar_str:\n",
    "        # If there's a space, it's likely that the exponent part is missing\n",
    "        # We'll use a default exponent of -4 in this case\n",
    "        bstar_str = bstar_str.replace(' ', '0')  # Replace the space with '0'\n",
    "        exponent = 'E-4'\n",
    "    else:\n",
    "        # If there's no space, the last two characters should represent the exponent\n",
    "        exponent = 'E' + bstar_str[-2:]\n",
    "        bstar_str = bstar_str[:-2]\n",
    "\n",
    "    # Ensure the BSTAR string starts with a sign\n",
    "    if bstar_str[0] not in ['+', '-']:\n",
    "        bstar_str = '+' + bstar_str\n",
    "\n",
    "    # Reconstruct the BSTAR value in scientific notation\n",
    "    bstar_formatted = f\"{bstar_str[0]}0.{bstar_str[1:]}{exponent}\"\n",
    "\n",
    "    return float(bstar_formatted)\n",
    "\n",
    "def extract_tle_features(df):\n",
    "    df['Arg_of_Perigee'] = df['TLE_LINE2'].str.slice(34, 42).astype(float)\n",
    "    df['Mean_Anomaly'] = df['TLE_LINE2'].str.slice(43, 51).astype(float)\n",
    "    df['BSTAR'] = df['TLE_LINE1'].apply(extract_bstar)\n",
    "    df['Rev_at_Epoch'] = df['TLE_LINE2'].str.slice(63, 68).astype(float)\n",
    "    return df\n",
    "\n",
    "\n",
    "filtered_df = extract_tle_features(filtered_df)\n",
    "\n",
    "filtered_df['Timestamp'] = pd.to_datetime(filtered_df['Timestamp'])\n",
    "filtered_df.sort_values(by=['NORAD_CAT_ID', 'Timestamp'], inplace=True)\n",
    "\n",
    "# Calculate the first derivative of Mean Motion\n",
    "filtered_df['Mean_Motion_Dot'] = filtered_df.groupby('NORAD_CAT_ID')['Mean_Motion'].diff() / filtered_df.groupby('NORAD_CAT_ID')['Timestamp'].diff().dt.total_seconds()\n",
    "\n",
    "# Calculate the second derivative of Mean Motion\n",
    "filtered_df['Mean_Motion_Dot_Dot'] = filtered_df.groupby('NORAD_CAT_ID')['Mean_Motion_Dot'].diff() / filtered_df.groupby('NORAD_CAT_ID')['Timestamp'].diff().dt.total_seconds()\n",
    "\n",
    "\n",
    "\n",
    "# Convert degrees to radians for cyclical transformation\n",
    "filtered_df['RAAN_rad'] = np.radians(filtered_df['RAAN'])\n",
    "filtered_df['Arg_of_Perigee_rad'] = np.radians(filtered_df['Arg_of_Perigee'])\n",
    "filtered_df['Mean_Anomaly_rad'] = np.radians(filtered_df['Mean_Anomaly'])\n",
    "\n",
    "# Cyclical transformations\n",
    "filtered_df['RAAN_cos'] = np.cos(filtered_df['RAAN_rad'])\n",
    "filtered_df['RAAN_sin'] = np.sin(filtered_df['RAAN_rad'])\n",
    "filtered_df['Arg_of_Perigee_cos'] = np.cos(filtered_df['Arg_of_Perigee_rad'])\n",
    "filtered_df['Arg_of_Perigee_sin'] = np.sin(filtered_df['Arg_of_Perigee_rad'])\n",
    "filtered_df['Mean_Anomaly_cos'] = np.cos(filtered_df['Mean_Anomaly_rad'])\n",
    "filtered_df['Mean_Anomaly_sin'] = np.sin(filtered_df['Mean_Anomaly_rad'])\n",
    "\n",
    "\n",
    "def calculate_rate_of_change(df, feature_name):\n",
    "    # Calculate the difference in the feature between consecutive rows\n",
    "    df[feature_name + '_Diff'] = df.groupby('NORAD_CAT_ID')[feature_name].diff()\n",
    "    \n",
    "    # Calculate the difference in time between consecutive rows in hours\n",
    "    df['Time_Diff_Hours'] = df.groupby('NORAD_CAT_ID')['Timestamp'].diff().dt.total_seconds() / 3600\n",
    "    \n",
    "    # Calculate the rate of change as feature difference divided by time difference\n",
    "    df[feature_name + '_Rate_of_Change'] = df[feature_name + '_Diff'] / df['Time_Diff_Hours']\n",
    "    \n",
    "    # Fill NaN values with 0 for the rate of change\n",
    "    df[feature_name + '_Rate_of_Change'] = df[feature_name + '_Rate_of_Change'].fillna(0)\n",
    "    \n",
    "    # Clean up by dropping the intermediate calculation columns\n",
    "    df.drop(columns=[feature_name + '_Diff', 'Time_Diff_Hours'], inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Features to calculate the rate of change for\n",
    "features_to_calculate = [\n",
    "    'Altitude', 'Mean_Motion', 'Eccentricity', 'Vel_Latitude', 'Vel_Longitude', 'Vel_Altitude'\n",
    "]\n",
    "\n",
    "# Calculate the rate of change for each feature in the list\n",
    "for feature in features_to_calculate:\n",
    "    filtered_df = calculate_rate_of_change(filtered_df, feature)\n",
    "\n",
    "\n",
    "filtered_df.head()\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Convert 'Timestamp' to datetime if it's not already\n",
    "filtered_df['Timestamp'] = pd.to_datetime(filtered_df['Timestamp'])\n",
    "\n",
    "# Now that 'Timestamp' is a datetime object, you can access .dt attributes\n",
    "filtered_df['Day_of_Year'] = filtered_df['Timestamp'].dt.dayofyear\n",
    "\n",
    "# Cyclical transformation for day of the year\n",
    "filtered_df['Day_of_Year_sin'] = np.sin(2 * np.pi * filtered_df['Day_of_Year'] / 365)\n",
    "filtered_df['Day_of_Year_cos'] = np.cos(2 * np.pi * filtered_df['Day_of_Year'] / 365)\n",
    "\n",
    "\n",
    "filtered_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "filtered_df.fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a220baf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2b789f",
   "metadata": {},
   "source": [
    "## GDBT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7602d291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting each Norad id into train,val and test\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Initialize empty lists to store DataFrames for training, validation, and test sets\n",
    "train_dfs = []\n",
    "val_dfs = []\n",
    "test_dfs = []\n",
    "\n",
    "# Iterate over each unique NORAD ID\n",
    "for norad_id in filtered_df['NORAD_CAT_ID'].unique():\n",
    "    # Filter the DataFrame for the current NORAD ID\n",
    "    norad_df = filtered_df[filtered_df['NORAD_CAT_ID'] == norad_id]\n",
    "    \n",
    "    # Split the data for this NORAD ID into training and temp sets (temp combines validation and test)\n",
    "    train_temp, temp = train_test_split(norad_df, test_size=0.4, random_state=42)\n",
    "    \n",
    "    # Split the temp set into validation and test sets\n",
    "    val, test = train_test_split(temp, test_size=0.5, random_state=42)\n",
    "    \n",
    "    \n",
    "    train_dfs.append(train_temp)\n",
    "    val_dfs.append(val)\n",
    "    test_dfs.append(test)\n",
    "\n",
    "# Concatenate the lists of DataFrames to form final training, validation, and test DataFrames\n",
    "final_train_df = pd.concat(train_dfs, ignore_index=True)\n",
    "val_df = pd.concat(val_dfs, ignore_index=True)\n",
    "test_df = pd.concat(test_dfs, ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df70ebdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting 'Epoch_Date' to ordinal number\n",
    "\n",
    "final_train_df['Epoch_Date'] = final_train_df['Epoch_Date'].apply(lambda x: x.toordinal())\n",
    "val_df['Epoch_Date'] = val_df['Epoch_Date'].apply(lambda x: x.toordinal())\n",
    "test_df['Epoch_Date'] = test_df['Epoch_Date'].apply(lambda x: x.toordinal())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1975574",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Selecting features and targets\n",
    "feature_columns = ['Mean_Motion', 'Eccentricity', 'Semi_Major_Axis', 'Orbital_Period', \n",
    "                   'Inclination', 'RAAN', 'Arg_of_Perigee', 'Mean_Motion_Dot', 'Mean_Motion_Dot_Dot',\n",
    "                   'RAAN_rad', 'Arg_of_Perigee_rad', 'Mean_Anomaly_rad', 'RAAN_cos', 'RAAN_sin',\n",
    "                   'Arg_of_Perigee_cos', 'Arg_of_Perigee_sin', 'Mean_Anomaly_cos', 'Mean_Anomaly_sin',\n",
    "                   'Altitude_Rate_of_Change', 'Mean_Motion_Rate_of_Change', 'Eccentricity_Rate_of_Change',\n",
    "                   'Vel_Latitude_Rate_of_Change', 'Vel_Longitude_Rate_of_Change', 'Vel_Altitude_Rate_of_Change',\n",
    "                   'Day_of_Year_sin', 'Day_of_Year_cos']\n",
    "\n",
    "\n",
    "# Replace infinite values with NaN\n",
    "final_train_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "val_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "test_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Impute NaN values with the median of the column\n",
    "for column in feature_columns:\n",
    "    median_value = final_train_df[column].median()\n",
    "    final_train_df[column].fillna(median_value, inplace=True)\n",
    "    val_df[column].fillna(median_value, inplace=True)\n",
    "    test_df[column].fillna(median_value, inplace=True)\n",
    "\n",
    "# Normalize feature columns\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(final_train_df[feature_columns])\n",
    "X_val_scaled = scaler.transform(val_df[feature_columns])\n",
    "X_test_scaled = scaler.transform(test_df[feature_columns])\n",
    "\n",
    "# Define target columns\n",
    "target_columns = ['Mean_Motion', 'Eccentricity', 'Inclination', 'RAAN', 'Arg_of_Perigee']\n",
    "\n",
    "# Extract target values\n",
    "Y_train = final_train_df[target_columns].values\n",
    "Y_val = val_df[target_columns].values\n",
    "Y_test = test_df[target_columns].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d205478d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the shapes are right\n",
    "\n",
    "print(\"Shape of X_train_scaled:\", X_train_scaled.shape)\n",
    "print(\"Shape of X_val_scaled:\", X_val_scaled.shape)\n",
    "print(\"Shape of X_test_scaled:\", X_test_scaled.shape)\n",
    "\n",
    "\n",
    "\n",
    "print(\"First 5 rows of X_train_scaled:\")\n",
    "print(X_train_scaled[:5])\n",
    "\n",
    "\n",
    "print(\"\\nFirst 5 rows of X_val_scaled:\")\n",
    "print(X_val_scaled[:5])\n",
    "\n",
    "\n",
    "print(\"\\nFirst 5 rows of X_test_scaled:\")\n",
    "print(X_test_scaled[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2f3ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Define your feature and target columns if not already done\n",
    "feature_columns = ['Mean_Motion_Dot', 'Mean_Motion_Dot_Dot', 'RAAN_rad',\n",
    "                   'Arg_of_Perigee_rad', 'Mean_Anomaly_rad', 'RAAN_cos', 'RAAN_sin',\n",
    "                   'Arg_of_Perigee_cos', 'Arg_of_Perigee_sin', 'Mean_Anomaly_cos',\n",
    "                   'Mean_Anomaly_sin', 'Altitude_Rate_of_Change', 'Mean_Motion_Rate_of_Change',\n",
    "                   'Eccentricity_Rate_of_Change', 'Vel_Latitude_Rate_of_Change', 'Vel_Longitude_Rate_of_Change',\n",
    "                   'Vel_Altitude_Rate_of_Change', 'Day_of_Year_sin', 'Day_of_Year_cos','Mean_Motion', 'Eccentricity', 'Inclination', 'RAAN', 'Arg_of_Perigee']\n",
    "\n",
    "# Define target variables\n",
    "target_columns = ['Latitude', 'Longitude', 'Altitude']\n",
    "\n",
    "# Split the data into features and target\n",
    "X = filtered_df[feature_columns]\n",
    "y = filtered_df[target_columns]\n",
    "\n",
    "# Split into train, validation, and test sets\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.1, random_state=42)  # Test split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.1, random_state=42)  # Validation split\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "final_mse,final_rmse,final_mae = 0,0,0\n",
    "\n",
    "# Initialize the model\n",
    "gbdt_model = lgb.LGBMRegressor()\n",
    "\n",
    "# Train and evaluate the model for each target variable\n",
    "for target in target_columns:\n",
    "    # Train the model\n",
    "    gbdt_model.fit(X_train_scaled, y_train[target])\n",
    "    \n",
    "    # Predict on the validation set\n",
    "    y_val_pred = gbdt_model.predict(X_val_scaled)\n",
    "    val_mse = mean_squared_error(y_val[target], y_val_pred)\n",
    "    val_rmse = np.sqrt(val_mse)\n",
    "    val_mae = mean_absolute_error(y_val[target], y_val_pred)\n",
    "    print(f\"Validation MSE: {val_mse}, RMSE: {val_rmse}, MAE: {val_mae}\")\n",
    "\n",
    "    # Predict on the test set\n",
    "    y_test_pred = gbdt_model.predict(X_test_scaled)\n",
    "    test_mse = mean_squared_error(y_test[target], y_test_pred)\n",
    "    test_rmse = np.sqrt(test_mse)\n",
    "    test_mae = mean_absolute_error(y_test[target], y_test_pred)\n",
    "\n",
    "    # Aggregate final metrics\n",
    "    final_mse += test_mse\n",
    "    final_rmse += test_rmse\n",
    "    final_mae += test_mae\n",
    "\n",
    "    # Plotting Actual vs. Predicted Values for the validation set\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(y_val[target], y_val_pred, alpha=0.3)\n",
    "    plt.plot([y_val[target].min(), y_val[target].max()], [y_val[target].min(), y_val[target].max()], 'k--', lw=2)\n",
    "    plt.xlabel(f'Actual {target}')\n",
    "    plt.ylabel(f'Predicted {target}')\n",
    "    plt.title(f'Validation - Actual vs Predicted {target}')\n",
    "    plt.show()\n",
    "\n",
    "# Calculate the average final metrics\n",
    "num_targets = len(target_columns)\n",
    "avg_final_mse = final_mse / num_targets\n",
    "avg_final_rmse = final_rmse / num_targets\n",
    "avg_final_mae = final_mae / num_targets\n",
    "\n",
    "# Print the average final metrics\n",
    "print(f\"Average Final Metrics on Test Set:\\nMSE: {avg_final_mse}\\nRMSE: {avg_final_rmse}\\nMAE: {avg_final_mae}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095b1fd2",
   "metadata": {},
   "source": [
    "## Kalman Filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ec2d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from filterpy.kalman import UnscentedKalmanFilter, MerweScaledSigmaPoints\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import gaussian_kde, zscore\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Normalize input features\n",
    "scaler = StandardScaler()\n",
    "scaled_columns = scaler.fit_transform(filtered_df[['Latitude', 'Longitude', 'Altitude']])\n",
    "filtered_df[['Latitude_scaled', 'Longitude_scaled', 'Altitude_scaled']] = scaled_columns\n",
    "\n",
    "# Initialize Unscented Kalman Filter\n",
    "initial_state_mean = np.zeros(6)\n",
    "initial_state_covariance = np.eye(6)\n",
    "sigma_points = MerweScaledSigmaPoints(n=6, alpha=1.0, beta=2.0, kappa=0.0)\n",
    "ukf = UnscentedKalmanFilter(dim_x=6, dim_z=3, dt=1., fx=lambda x, dt: np.dot(np.eye(6), x), hx=lambda x: x[:3], points=sigma_points)\n",
    "ukf.x = initial_state_mean\n",
    "ukf.P = initial_state_covariance\n",
    "\n",
    "# Collect all predictions and actual measurements\n",
    "all_predicted_states = []\n",
    "all_actual_measurements = []\n",
    "\n",
    "for norad_id, data in filtered_df.groupby('NORAD_CAT_ID'):\n",
    "    predicted_states = []\n",
    "    actual_measurements = []\n",
    "\n",
    "    for _, row in data.iterrows():\n",
    "        ukf.predict()\n",
    "        ukf.update(row[['Latitude_scaled', 'Longitude_scaled', 'Altitude_scaled']].to_numpy())\n",
    "        estimated_state = ukf.x[:3]\n",
    "        actual_state = row[['Latitude_scaled', 'Longitude_scaled', 'Altitude_scaled']].to_numpy()\n",
    "        predicted_states.append(estimated_state)\n",
    "        actual_measurements.append(actual_state)\n",
    "\n",
    "    all_predicted_states.extend(predicted_states)\n",
    "    all_actual_measurements.extend(actual_measurements)\n",
    "\n",
    "# Convert lists to numpy arrays for easier manipulation\n",
    "all_predicted_array = np.array(all_predicted_states)\n",
    "all_actual_array = np.array(all_actual_measurements)\n",
    "\n",
    "# Inverse transform the predictions and actuals back to their original scale\n",
    "all_predicted_array = scaler.inverse_transform(all_predicted_array)\n",
    "all_actual_array = scaler.inverse_transform(all_actual_array)\n",
    "\n",
    "# Calculate evaluation metrics on the inverse transformed data\n",
    "mse = mean_squared_error(all_actual_array, all_predicted_array, multioutput='raw_values')\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(all_actual_array, all_predicted_array, multioutput='raw_values')\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"Evaluation Metrics on Inverse Transformed Data:\")\n",
    "print(\"MSE (Latitude, Longitude, Altitude):\", mse)\n",
    "print(\"RMSE (Latitude, Longitude, Altitude):\", rmse)\n",
    "print(\"MAE (Latitude, Longitude, Altitude):\", mae)\n",
    "\n",
    "# Plot actual vs predicted values using the inverse transformed data\n",
    "plt.figure(figsize=(18, 6))\n",
    "titles = ['Latitude', 'Longitude', 'Altitude']\n",
    "colors = ['blue', 'green', 'red']\n",
    "for i in range(3):\n",
    "    plt.subplot(1, 3, i+1)\n",
    "    sns.scatterplot(x=all_actual_array[:, i], y=all_predicted_array[:, i], alpha=0.5, color=colors[i])\n",
    "    plt.plot([all_actual_array[:, i].min(), all_actual_array[:, i].max()], [all_actual_array[:, i].min(), all_actual_array[:, i].max()], 'r--', color='black')\n",
    "    plt.title(f'Actual vs Predicted {titles[i]}')\n",
    "    plt.xlabel(f'Actual {titles[i]}')\n",
    "    plt.ylabel(f'Predicted {titles[i]}')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd12e682",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e816ebec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.layers import LSTM, Dropout, BatchNormalization, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.regularizers import l1\n",
    "from keras.layers import Bidirectional, Attention\n",
    "from keras.layers import GlobalAveragePooling1D\n",
    "\n",
    "# Define features and targets\n",
    "features_list = ['Mean_Motion', 'Eccentricity','Semi_Major_Axis', 'Orbital_Period', 'Apogee', 'Perigee', 'Inclination','RAAN']\n",
    "features_list += ['Epoch_Year', 'Epoch_Day', 'hour_sin', 'hour_cos', 'day_of_week_sin', 'day_of_week_cos']\n",
    "targets = ['Latitude', 'Longitude']\n",
    "\n",
    "# Split data into training, validation, and testing sets\n",
    "train_end = int(0.7 * len(norad_data))\n",
    "val_end = train_end + int(0.15 * len(norad_data))\n",
    "\n",
    "train_df = norad_data.iloc[:train_end]\n",
    "val_df = norad_data.iloc[train_end:val_end]\n",
    "test_df = norad_data.iloc[val_end:]\n",
    "\n",
    "# Scale features and targets\n",
    "feature_scaler = StandardScaler()\n",
    "target_scaler = StandardScaler()\n",
    "\n",
    "train_features = feature_scaler.fit_transform(train_df[features_list])\n",
    "val_features = feature_scaler.transform(val_df[features_list])\n",
    "test_features = feature_scaler.transform(test_df[features_list])\n",
    "\n",
    "train_targets = target_scaler.fit_transform(train_df[targets])\n",
    "val_targets = target_scaler.transform(val_df[targets])\n",
    "test_targets = target_scaler.transform(test_df[targets])\n",
    "\n",
    "# Reshape features for LSTM input\n",
    "train_features = train_features.reshape((train_features.shape[0], 1, train_features.shape[1]))\n",
    "val_features = val_features.reshape((val_features.shape[0], 1, val_features.shape[1]))\n",
    "test_features = test_features.reshape((test_features.shape[0], 1, test_features.shape[1]))\n",
    "\n",
    "features_input = Input(shape=(1, 14))\n",
    "\n",
    "lstm_out = Bidirectional(LSTM(100, return_sequences=True, kernel_regularizer=l2(0.5)))(features_input)\n",
    "lstm_out = Dropout(0.5)(lstm_out)\n",
    "lstm_out = BatchNormalization()(lstm_out)\n",
    "\n",
    "lstm_out = Bidirectional(LSTM(100, return_sequences=True, kernel_regularizer=l2(0.5)))(lstm_out)\n",
    "lstm_out = Dropout(0.5)(lstm_out)\n",
    "lstm_out = BatchNormalization()(lstm_out)\n",
    "\n",
    "lstm_out = Bidirectional(LSTM(100, return_sequences=True, kernel_regularizer=l2(0.5)))(lstm_out)\n",
    "lstm_out = Dropout(0.5)(lstm_out)\n",
    "lstm_out = BatchNormalization()(lstm_out)\n",
    "\n",
    "lstm_out = Bidirectional(LSTM(100, kernel_regularizer=l2(0.5)))(lstm_out)\n",
    "lstm_out = Dropout(0.5)(lstm_out)\n",
    "lstm_out = BatchNormalization()(lstm_out)\n",
    "\n",
    "\n",
    "lstm_out = Dense(128, activation='relu')(lstm_out)\n",
    "lstm_out = Dropout(0.5)(lstm_out)\n",
    "lstm_out = BatchNormalization()(lstm_out)\n",
    "\n",
    "output = Dense(2, activation='linear')(lstm_out)\n",
    "\n",
    "model3 = Model(inputs=features_input, outputs=output)\n",
    "\n",
    "# Compile model\n",
    "model3.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
    "\n",
    "# Define callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, min_lr=1e-6, verbose=1)\n",
    "\n",
    "# Train model\n",
    "history = model3.fit(\n",
    "    train_features,\n",
    "    train_targets,\n",
    "    epochs=20,\n",
    "    validation_data=(val_features, val_targets),\n",
    "    callbacks=[early_stopping, reduce_lr],\n",
    "    batch_size=64,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# model summary\n",
    "model3.summary()\n",
    "\n",
    "\n",
    "# 1. Plot training and validation loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de85eea1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
